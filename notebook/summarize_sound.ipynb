{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/code_Bao/stock_price_4_fun/notebook\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "try:\n",
    "    print(file_path)\n",
    "except:\n",
    "    file_path = os.path.abspath('')\n",
    "    os.chdir(os.path.dirname(file_path))\n",
    "    print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device: cuda\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import urllib3\n",
    "\n",
    "urllib3.disable_warnings()\n",
    "from src.summarize_text import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device: cuda\n"
     ]
    }
   ],
   "source": [
    "speech_to_text = SpeechSummaryProcessor(audio_path='data/audio_ogg.ogg')\n",
    "text = speech_to_text.generate_speech_to_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"1: I'll be home later at 8 o'clock to pack my clothes\\n\\n2: At 9 o'clock I go to bed to write my essay and at 10 o'clock I go to bed\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = 'SSI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class NewsScraper:\n",
    "    '''\n",
    "    Scape News from https://vnexpress.net/ \n",
    "    How to run selenium on linux\n",
    "    https://cloudbytes.dev/snippets/run-selenium-and-chrome-on-wsl2#:~:text=With%20Selenium%20libraries%2C%20Python%20can,using%20Python%20and%20Selenium%20webdriver.\n",
    "    \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def search_stock_news(self, symbol:str = \"SSI\",\n",
    "                          date_format:Literal['day', 'week', 'month', 'year']='day')-> list : \n",
    "        symbol = symbol.upper()\n",
    "        url = f\"https://timkiem.vnexpress.net/?search_f=&q={symbol}&date_format={date_format}&\"\n",
    "\n",
    "        # Send a GET request to the webpage\n",
    "        response = requests.get(url)\n",
    "        # Check if the request was successful (status code 200)\n",
    "        attemp = 0\n",
    "        max_attemps = 3\n",
    "        news_urls = []\n",
    "\n",
    "        while attemp <= max_attemps:\n",
    "            try:\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                # Assuming the news articles are wrapped in <article> elements\n",
    "                articles = soup.find_all('article', class_='item-news-common')\n",
    "                # Iterate through each article and extract the URL\n",
    "                for article in articles:\n",
    "                    url = article.get('data-url')\n",
    "                    if url:\n",
    "                        news_urls.append(url)\n",
    "\n",
    "                break\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {str(e)}\")\n",
    "                print(\"Resetting the Scraper in 10 seconds...\")\n",
    "                time.sleep(10)  \n",
    "                attemp += 1\n",
    "                if attemp > max_attemps:\n",
    "                    print(\"Max attempts reached. Exiting.\")\n",
    "                    break\n",
    "\n",
    "        return news_urls\n",
    "\n",
    "    def search_top_news_cafef(self)-> list:\n",
    "\n",
    "        url = 'https://cafef.vn/'\n",
    "        # Send a GET request to the webpage\n",
    "        response = requests.get(url)\n",
    "        # Check if the request was successful (status code 200)\n",
    "        attemp = 0\n",
    "        max_attemps = 3\n",
    "        news_urls = []\n",
    "\n",
    "        while attemp <= max_attemps:\n",
    "            try:\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "                # Find top news elements\n",
    "                top_news_elements = soup.find_all('div', class_='top_noibat')\n",
    "                top_news = top_news_elements[0] if top_news_elements else None\n",
    "\n",
    "                top_news_links = top_news.find_all('a')\n",
    "                # Extract and print the links\n",
    "                for link in top_news_links:\n",
    "                    news_link = link.get('href')\n",
    "                    news_link = f'{url}{news_link}'\n",
    "                    news_urls.append(news_link)\n",
    "\n",
    "                    # Convert set to list\n",
    "                    news_urls = list(set(news_urls))\n",
    "                break\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {str(e)}\")\n",
    "                print(\"Resetting the Scraper in 10 seconds...\")\n",
    "                time.sleep(10)  \n",
    "                attemp += 1\n",
    "                if attemp > max_attemps:\n",
    "                    print(\"Max attempts reached. Exiting.\")\n",
    "                    break\n",
    "\n",
    "        return news_urls\n",
    "    \n",
    "    def search_top_news_vnexpress(self)-> list:\n",
    "\n",
    "        url = 'https://vnexpress.net/kinh-doanh'\n",
    "        # Send a GET request to the webpage\n",
    "        response = requests.get(url)\n",
    "        # Check if the request was successful (status code 200)\n",
    "        attemp = 0\n",
    "        max_attemps = 3\n",
    "        news_urls = []\n",
    "\n",
    "        while attemp <= max_attemps:\n",
    "            try:\n",
    "                # Parse the HTML content\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "                # Find news elements\n",
    "                news_elements = soup.find_all('h3', class_='title_news')\n",
    "                news_elements.append(soup.find('section', class_='section_topstory_folder'))\n",
    "\n",
    "                # Extract and return the URLs\n",
    "                news_urls = [element.find('a').get('href') for element in news_elements if element.find('a')]\n",
    "                break\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {str(e)}\")\n",
    "                print(\"Resetting the Scraper in 10 seconds...\")\n",
    "                time.sleep(10)  \n",
    "                attemp += 1\n",
    "                if attemp > max_attemps:\n",
    "                    print(\"Max attempts reached. Exiting.\")\n",
    "                    break\n",
    "\n",
    "        return news_urls\n",
    "    \n",
    "    def search_top_news(self)-> list:\n",
    "        news_urls = self.search_top_news_cafef()\n",
    "        news_urls.extend(self.search_top_news_vnexpress())\n",
    "        return news_urls\n",
    "    \n",
    "    def take_text_from_link(self, news_url:str) -> str :\n",
    "        news_url = [news_url]\n",
    "        loader = NewsURLLoader(urls=news_url, \n",
    "                            post_processors=[clean_extra_whitespace],)\n",
    "        news_text = loader.load()\n",
    "        news_text = news_text[0].page_content\n",
    "        return  news_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_scraper = NewsScraper()\n",
    "# news_list = news_scraper.search_stock_news(symbol=symbol, date_format='year')\n",
    "news_list = news_scraper.search_top_news()\n",
    "news = news_scraper.take_text_from_link(news_url=news_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://cafef.vn//khoi-ngoai-quay-dau-ban-rong-nghin-ty-trong-tuan-dau-tien-nam-2024-dau-la-tam-diem-xa-hang-188240106022231686.chn',\n",
       " 'https://cafef.vn//thi-phan-moi-gioi-hose-quy-4-2023-tcbs-vuot-vndirect-hsc-day-mbs-ra-khoi-top-5-188240105183615455.chn',\n",
       " 'https://cafef.vn//ong-pham-nhat-vuong-lam-tgd-vinfast-188240106100255167.chn',\n",
       " 'https://cafef.vn//ngay-nay-2-nam-truoc-vn-index-lap-dinh-lich-su-152857-diem-188240105233819823.chn',\n",
       " 'https://cafef.vn//thi-truong-bat-dong-san-khat-nguon-cung-gia-nha-tang-lien-tuc-du-bao-kho-giam-188240106065023587.chn',\n",
       " 'https://vnexpress.net/dai-gia-xang-dau-mien-tay-no-thue-hon-1-000-ty-dong-4697649.html',\n",
       " 'https://vnexpress.net/ong-pham-nhat-vuong-truc-tiep-dieu-hanh-vinfast-4697659.html',\n",
       " 'https://vnexpress.net/dai-gia-tai-chinh-trung-quoc-nop-don-xin-pha-san-4697608.html',\n",
       " 'https://vnexpress.net/lam-phat-chau-au-tang-toc-tro-lai-sau-8-thang-4697666.html']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thị trường chứng khoán Việt Nam ghi nhận tuần giao dịch đầu năm 2024 tương đối khởi sắc. VN-Index thậm chí có thời điểm vượt mốc 1.160 điểm trước khi kết tuần giữ vững mốc 1.150 với thanh khoản dồi dào. Dòng tiền nhiều phiên đặc biệt tại nhóm vốn hoá lớn như ngân hàng. Kết tuần, chỉ số chính tăng 2,19% so với tuần cuối năm 2023 lên mức 1.154,68 điểm.\\n\\nDù vậy, giao dịch khối ngoại lại không mấy khả quan. Sau tuần cuối năm trở lại mua ròng, nhà đầu tư nước ngoài nhanh chóng quay đầu bán ròng trong tuần giao dịch đầu năm 2024. Giá trị bán ròng cả tuần 2-5/1 đạt 1.190 tỷ đồng, trong đó bán ròng khớp lệnh 705 tỷ và bán ròng thoả thuận 486 tỷ đồng.\\n\\nXét riêng trên từng sàn trong tuần, nhà đầu tư nước ngoài bán ròng luỹ kế 998 tỷ đồng trên HoSE, bán ròng 24 tỷ đồng trên HNX và bán ròng 168 tỷ đồng trên sàn UPCoM.\\n\\nThống kê trên từng mã, hai chứng chỉ quỹ FUESSVFL và FUEVFVND lọt top bị khối ngoại bán ròng mạnh nhất tuần đầu năm với giá trị lần lượt đạt 396 tỷ và 185 tỷ đồng. Bên cạnh đó, VHM bị bán ròng với giá trị 220 tỷ đồng, SHB cũng bị bán ròng 100 tỷ đồng.\\n\\nCổ phiếu NCG trên sàn UPCoM cũng bị nhà đầu tư ngoại xả ròng gần 100 tỷ đồng, theo sau là VRE bị bán ròng 99,5 tỷ đồng. Danh sách bán ròng của khối ngoại trong tuần qua còn có MSN, DXG, PVS,...\\n\\nChiều ngược lại, cổ phiếu chứng khoán VCB được khối ngoại mua ròng mạnh nhất với 316 tỷ đồng. Bên cạnh đó, hai cổ phiếu VPB và VHC lần lượt được mua ròng 138 tỷ đồng và 93 tỷ đồng trong 5 phiên của tuần qua. Khối ngoại cũng mua ròng mạnh tại cổ phiếu IDC và OCB với 53 tỷ đồng và 50 tỷ. Lực mua ròng trong tuần qua còn ghi nhận MSB, ASM, CTG, VCI...'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thị trường chứng khoán Việt Nam ghi nhận tuần giao dịch đầu năm 2024 tương đối khởi sắc. VN-Index thậm chí có thời điểm vượt mốc 1.160 điểm trước khi kết tuần giữ vững mốc 1.150 với thanh khoản dồi dào. Dòng tiền nhiều phiên đặc biệt tại nhóm vốn hoá lớn như ngân hàng. Kết tuần, chỉ số chính tăng 2,19% so với tuần cuối năm 2023 lên mức 1.154,68 điểm.\\n\\nDù vậy, giao dịch khối ngoại lại không mấy khả quan. Sau tuần cuối năm trở lại mua ròng, nhà đầu tư nước ngoài nhanh chóng quay đầu bán ròng trong tuần giao dịch đầu năm 2024. Giá trị bán ròng cả tuần 2-5/1 đạt 1.190 tỷ đồng, trong đó bán ròng khớp lệnh 705 tỷ và bán ròng thoả thuận 486 tỷ đồng.\\n\\nXét riêng trên từng sàn trong tuần, nhà đầu tư nước ngoài bán ròng luỹ kế 998 tỷ đồng trên HoSE, bán ròng 24 tỷ đồng trên HNX và bán ròng 168 tỷ đồng trên sàn UPCoM.\\n\\nThống kê trên từng mã, hai chứng chỉ quỹ FUESSVFL và FUEVFVND lọt top bị khối ngoại bán ròng mạnh nhất tuần đầu năm với giá trị lần lượt đạt 396 tỷ và 185 tỷ đồng. Bên cạnh đó, VHM bị bán ròng với giá trị 220 tỷ đồng, SHB cũng bị bán ròng 100 tỷ đồng.\\n\\nCổ phiếu NCG trên sàn UPCoM cũng bị nhà đầu tư ngoại xả ròng gần 100 tỷ đồng, theo sau là VRE bị bán ròng 99,5 tỷ đồng. Danh sách bán ròng của khối ngoại trong tuần qua còn có MSN, DXG, PVS,...\\n\\nChiều ngược lại, cổ phiếu chứng khoán VCB được khối ngoại mua ròng mạnh nhất với 316 tỷ đồng. Bên cạnh đó, hai cổ phiếu VPB và VHC lần lượt được mua ròng 138 tỷ đồng và 93 tỷ đồng trong 5 phiên của tuần qua. Khối ngoại cũng mua ròng mạnh tại cổ phiếu IDC và OCB với 53 tỷ đồng và 50 tỷ. Lực mua ròng trong tuần qua còn ghi nhận MSB, ASM, CTG, VCI...'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 230, but your input_length is only 19. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=9)\n"
     ]
    }
   ],
   "source": [
    "new_summarizer = NewsSummarizer()\n",
    "sum_text = new_summarizer.summary_news(news= news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSI Research cho rằng việc phát hành tín phiếu điều chỉnh thanh khoản ngắn hạn không có nghĩa là Ngân hàng Nhà nước sẽ thắt chặt chính sách tiền tệ. \n",
      "Khối lượng phát hành lần đầu đạt 10.000 tỷ đồng trong 3 phiên đầu tiên. \n",
      "Trên thực tế, việc phát hành tín phiếu kho bạc cũng có thể coi là tích cực.\n",
      "\n",
      "Tăng trưởng tín dụng tính đến ngày 15/9 chỉ ở mức 5,5% so với đầu năm. \n",
      "Ngân hàng Nhà nước chọn phương án phát hành tín phiếu kho bạc bắt đầu từ năm 2023 thay vì bán dự trữ ngoại hối như năm 2022. \n",
      "Điều này rất có thể là do hoạt động đầu cơ tỷ giá.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sum_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "watch_list=['GEX', 'VIX', 'HHV', 'BSR', 'SHS', 'PDR', 'CTR', 'HAH', 'VNINDEX', 'VND']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class StockNewsDatabase:\n",
    "    def __init__(self, summary_news_data_path='data/summary_stock_news.json'):\n",
    "        self.summary_news_data_path = summary_news_data_path\n",
    "        self.news_scraper = NewsScraper()\n",
    "        self.new_summarizer = NewsSummarizer()\n",
    "\n",
    "    def update_top_news(self):\n",
    "        '''Update the top news on schedule'''\n",
    "        summary_data = []\n",
    "\n",
    "        news_list = self.news_scraper.search_top_news()[:8]\n",
    "        for news_url in news_list:\n",
    "            news = self.news_scraper.take_text_from_link(news_url=news_url)\n",
    "            sum_text = self.new_summarizer.summary_news(news=news)\n",
    "\n",
    "            # Append data to summary_data list\n",
    "            summary_data.append({\n",
    "                \"type\": \"top_news\",\n",
    "                \"stock\": None,\n",
    "                \"summary_text\": sum_text,\n",
    "                \"news_url\": news_url\n",
    "            })\n",
    "\n",
    "        # Save the summary data to a JSON file\n",
    "        self._save_summary_data(summary_data)\n",
    "\n",
    "    def update_stock_news(self, watch_list):\n",
    "        '''Update the news summary on schedule'''\n",
    "        summary_data = []\n",
    "\n",
    "        for stock in watch_list:\n",
    "            news_list = self.news_scraper.search_stock_news(symbol=stock, date_format='month')[:1]\n",
    "            for news_url in news_list:\n",
    "                news = self.news_scraper.take_text_from_link(news_url=news_url)\n",
    "                sum_text = self.new_summarizer.summary_news(news=news)\n",
    "\n",
    "                # Append data to summary_data list\n",
    "                summary_data.append({\n",
    "                    \"type\": \"stock_news\",\n",
    "                    \"stock\": stock,\n",
    "                    \"summary_text\": sum_text,\n",
    "                    \"news_url\": news_url\n",
    "                })\n",
    "\n",
    "        # Save the summary data to a JSON file\n",
    "        self._save_summary_data(summary_data)\n",
    "\n",
    "    def read_summary_stock_news(self):\n",
    "        try:\n",
    "            with open(self.summary_news_data_path, 'r', encoding='utf-8') as json_file:\n",
    "                data = json.load(json_file)\n",
    "                return data\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: File {self.summary_news_data_path} not found.\")\n",
    "            return None\n",
    "\n",
    "    def extract_text_for_stock(self, stock_symbol):\n",
    "        summary_data = self.read_summary_stock_news()\n",
    "        if summary_data is None:\n",
    "            return None, None\n",
    "        \n",
    "        for entry in summary_data:\n",
    "            if entry[\"stock\"] == stock_symbol:\n",
    "                return entry[\"summary_text\"], entry[\"news_url\"]\n",
    "\n",
    "        print(f\"No summary text found for stock: {stock_symbol}\")\n",
    "        return None, None\n",
    "\n",
    "    def get_all_stocks(self):\n",
    "        summary_data = self.read_summary_stock_news()\n",
    "\n",
    "        if summary_data:\n",
    "            all_stocks = set(entry[\"stock\"] for entry in summary_data)\n",
    "            return list(all_stocks)\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    def _save_summary_data(self, summary_data):\n",
    "        check_path(self.summary_news_data_path)\n",
    "        with open(self.summary_news_data_path, 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(summary_data, json_file, ensure_ascii=False, indent=2)\n",
    "        print(f\"Summary data saved to {self.summary_news_data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_db = StockNewsDatabase()\n",
    "# news_db.update_stock_news(watch_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Cổ phiếu GMD của Gemadept là một trong những cổ phiếu tăng mạnh nhất sàn HoSE khi đóng cửa tăng toàn bộ lên 73.000 đồng. \\nCác cổ phiếu cảng biển, logistics khác cũng giao dịch sôi động như HAH tăng gần 4%. \\nChứng khoán cũng mở cửa phiên đầu tiên trong trạng thái tích cực.\\n\\nỞ nhóm ngân hàng, sắc xanh cũng chiếm ưu thế. \\nVPB, BID, CTG tăng gần 2%, VIB, HDB, TCB, STB tăng trên 1%. \\nNhóm bán lẻ và hàng tiêu dùng cũng nhận được sự quan tâm.\\n',\n",
       " 'https://vnexpress.net/co-phieu-van-tai-bien-noi-song-4693192.html')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_db.extract_text_for_stock(stock_symbol='HAH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VIX', 'HAH']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_db.get_all_stocks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 230, but your input_length is only 105. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=52)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary data saved to data/summary_stock_news.json\n"
     ]
    }
   ],
   "source": [
    "news_db.update_stock_news(watch_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/code_Bao/stock_price_4_fun/venv/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "Your max_length is set to 230, but your input_length is only 19. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=9)\n",
      "Your max_length is set to 230, but your input_length is only 66. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=33)\n",
      "Your max_length is set to 230, but your input_length is only 94. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=47)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary data saved to data/summary_stock_news.json\n"
     ]
    }
   ],
   "source": [
    "news_db.update_top_news()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
